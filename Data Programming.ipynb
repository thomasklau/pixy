{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Bag of Words\n",
    "\n",
    "For each of our classes, we compute the SIFT features and k-means cluster in order to get our \"visual words\". \n",
    "\n",
    "1) First we provide the directory of our images and split our dataset into train and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder tempresults created\n",
      "2016-12-07 02:36:52.997707| Finished configuring system\n",
      "2016-12-07 02:36:53.021363| Found classes and created split\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import phow_caltech101 as phow\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "from cPickle import dump, load\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pylab as pl\n",
    "\n",
    "#Define initial configuration setup variables\n",
    "IMAGEDIRECTORY = '101_ObjectCategories' #ATTENTION: SET IMAGE DIRECTORY HERE, MUST BE IN SAME FOLDER AS NOTEBOOK\n",
    "IDENTIFIER = 'newBatch' #The identifier is a string that we use to cache our datasets with - using an identifier allows us to retrieve things from our cache later\n",
    "OVERWRITE = False #Ignore the cache?\n",
    "NUMTRAIN = 15 #From each directory, how many images do we want to train with to find our visual words\n",
    "NUMTEST = 15 #From each directory, how many images do we want to use to test how good our visual words clustering is\n",
    "NUMCLASSES = 102 #How many folders do we have, aka how many visual word clusterings do we have to do\n",
    "NUMWORDS = 600 #How many words do we want to use to describe our image classes\n",
    "\n",
    "conf = phow.Configuration(IDENTIFIER)\n",
    "conf.setImagePath(IMAGEDIRECTORY)\n",
    "conf.setNumTrain(NUMTRAIN)\n",
    "conf.setNumTest(NUMTEST)\n",
    "conf.setNumClasses(NUMCLASSES)\n",
    "conf.setNumWords(NUMWORDS)\n",
    "\n",
    "print str(datetime.now()) + '| Finished configuring system'\n",
    "\n",
    "classes = phow.get_classes(conf.calDir, conf.numClasses)\n",
    "\n",
    "model = phow.Model(classes, conf)\n",
    "\n",
    "all_images, all_images_class_labels = phow.get_all_images(classes, conf)\n",
    "selTrain, selTest = phow.create_split(all_images, conf)\n",
    "\n",
    "print str(datetime.now()) + '| Found classes and created split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use our selTrain data set and find our visual bag of words, we've specified to find 600 visual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-07 02:36:55.573643| Start training vocabulary - launching threads to do SIFT\n",
      "2016-12-07 02:36:57.670664|Now getting visual words via k-means clustering, goes until convergence\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Train vocabulary\n",
    "##################\n",
    "print str(datetime.now()) + '| Start training vocabulary - launching threads to do SIFT'\n",
    "if (not exists(conf.vocabPath)) | OVERWRITE:\n",
    "    vocab = phow.trainVocab(selTrain, all_images, conf)\n",
    "    phow.savemat(conf.vocabPath, {'vocab': vocab})\n",
    "else:\n",
    "    print str(datetime.now()) + '| Done! Using old vocab from ' + conf.vocabPath\n",
    "    vocab = phow.loadmat(conf.vocabPath)['vocab']\n",
    "\n",
    "model.vocab = vocab #The columns of vocab are our visual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate amount of \"contribution\" each word has to a specific image. (Imagine that the vocab is a set of singular vectors and we're computing our singular values.)\n",
    "\n",
    "First, we pull out the SIFT vector from each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          |"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-07 02:40:51.877497| Computing Spatial Histograms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|##############                                                            |"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Compute spatial histograms\n",
    "############################\n",
    "print str(datetime.now()) + '| Computing Spatial Histograms'\n",
    "if (not exists(conf.histPath)) | OVERWRITE:\n",
    "    hists = phow.computeHistograms(all_images, model, conf, vocab)\n",
    "    phow.savemat(conf.histPath, {'hists': hists})\n",
    "else:\n",
    "    print str(datetime.now()) + '| Found old historams at:' + conf.histPath\n",
    "    hists = phow.loadmat(conf.histPath)['hists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we use the AdditiveChi2Sampler from sci-kit learn to do a feature mapping (analgously calculating the singular values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Compute feature map\n",
    "#####################\n",
    "print str(datetime.now()) + '| Computing Feature Map'\n",
    "transformer = AdditiveChi2Sampler()\n",
    "histst = transformer.fit_transform(hists)\n",
    "train_data = histst[selTrain]\n",
    "test_data = histst[selTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can also naively classify what each image is with an SVM. We will use this method as a naive baseline to see how well our data programming approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Train SVM\n",
    "###########\n",
    "if (not exists(conf.modelPath)) | OVERWRITE:\n",
    "    print str(datetime.now()) + '| Training liblinear svm'\n",
    "\n",
    "    clf = svm.LinearSVC(C=10) #C is the penalty parameter\n",
    "    print clf\n",
    "    \n",
    "    clf.fit(train_data, all_images_class_labels[selTrain])\n",
    "    \n",
    "    with open(conf.modelPath, 'wb') as fp:\n",
    "        dump(clf, fp)\n",
    "else:\n",
    "    print str(datetime.now()) + '| Loading old SVM model'\n",
    "    with open(conf.modelPath, 'rb') as fp:\n",
    "        clf = load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Test SVM\n",
    "##########\n",
    "if (not exists(conf.resultPath)) | OVERWRITE:\n",
    "    print str(datetime.now()) + '| Testing svm by outputting confusion matrix'\n",
    "    predicted_classes = clf.predict(test_data)\n",
    "    true_classes = all_images_class_labels[selTest]\n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    cm = confusion_matrix(predicted_classes, true_classes)\n",
    "    with open(conf.resultPath, 'wb') as fp:\n",
    "        dump(conf, fp)\n",
    "        dump(cm, fp)\n",
    "        dump(predicted_classes, fp)\n",
    "        dump(true_classes, fp)\n",
    "        dump(accuracy, fp)\n",
    "else:\n",
    "    with open(conf.resultPath, 'rb') as fp:\n",
    "        conf = load(fp)\n",
    "        cm = load(fp)\n",
    "        predicted_classes = load(fp)\n",
    "        true_classes = load(fp)\n",
    "        accuracy = load(fp)\n",
    "        \n",
    "pl.matshow(cm)\n",
    "pl.title('Confusion Matrix')\n",
    "pl.colorbar()\n",
    "pl.show()\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the confusion matrix, we get a decently good accuracy, around ~70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snorkel Installation and Setup\n",
    "\n",
    "First, let's load snorkel into our Jupyter python path. For some reason, the snorkel installation doesn't always work so this is here as a backup. Make sure that you follow the installation instructions in the README as well before executing any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#Set the correct environment variables\n",
    "os.environ['SNORKELHOME']='/home/thomas/snorkel'\n",
    "os.environ['PYTHONPATH']=':/home/thomas/snorkel:/home/thomas/snorkel/treedlib:/home/thomas/snorkel:/home/thomas/snorkel/treedlib'\n",
    "os.environ['PATH']='/home/thomas/bin:/home/thomas/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/thomas/snorkel:/home/thomas/snorkel/treedlib:/home/thomas/snorkel:/home/thomas/snorkel/treedlib'\n",
    "\n",
    "#Add python to the system path so that python can find the package\n",
    "sys.path.append('/home/thomas/snorkel')\n",
    "sys.path.append('/home/thomas/snorkel/treedlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Generative Model\n",
    "We estimate the accuracies of the labeling functions without supervision. Specifically, we estimate the parameters of a `NaiveBayes` generative model.\n",
    "\n",
    "First, we have to specify a sparse matrix with labeling function output. The setup of the matrix is as follows:\n",
    "\n",
    "Rows of the matrix correspond with individual test images\n",
    "Columns of the matrix correspond to individual labeling functions\n",
    "Entries in the matrix are {-1, 0, 1}, the possible outputs of each labeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup our sparse array\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "a = csr_matrix((3, 4), dtype=np.int8).toarray()\n",
    "\n",
    "a[0,1] = 1\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x4 sparse matrix of type '<type 'numpy.int8'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert our sparse array into a format that snorkel knows how to deal with\n",
    "from snorkel.annotations import csr_LabelMatrix\n",
    "from snorkel.annotations import csr_AnnotationMatrix\n",
    "\n",
    "a1 = csr_LabelMatrix(csr_AnnotationMatrix(a))\n",
    "\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t3\n",
      "Features:\t\t\t4\n",
      "================================================================================\n",
      "Begin training for rate=1e-05, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.000000\n",
      "SGD converged for mu=1e-06 after 10 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.999985,  0.999985,  0.999985,  0.999985])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(a1, n_iter=1000, rate=1e-5)\n",
    "\n",
    "gen_model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105563,  0.5       ,  0.5       ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(a1)\n",
    "\n",
    "train_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "from snorkel.learning_utils import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "iter_param = ListParameter('n_iter', [250, 500, 1000, 2000])\n",
    "rate_param = RangeParameter('rate', 1e-4, 1e-2, step=0.75, log_base=10)\n",
    "reg_param  = RangeParameter('mu', 1e-8, 1e-2, step=1, log_base=10)\n",
    "\n",
    "disc_model = LogReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher = RandomSearch(disc_model, F_train, train_marginals, 10, iter_param, rate_param, reg_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
